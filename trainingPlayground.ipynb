{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface decision transformer:https://huggingface.co/docs/transformers/en/model_doc/decision_transformer\n",
    "\n",
    "training huggingface decision transformer: https://github.com/huggingface/blog/blob/main/notebooks/101_train-decision-transformers.ipynb\n",
    "\n",
    "\n",
    "custom gym environments:https://www.gymlibrary.dev/content/environment_creation/\n",
    "\n",
    "gymnasium documentation: https://gymnasium.farama.org/\n",
    "\n",
    "decision transformer github: https://github.com/kzl/decision-transformer/tree/master\n",
    "\n",
    "very simple DT: https://github.com/nikhilbarhate99/min-decision-transformer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "\n",
    "from TensorRTS import TensorRTS\n",
    "# import tensorRTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import abc\n",
    "# from typing import Dict, List, Mapping, Tuple, Set\n",
    "# from entity_gym.env import Observation\n",
    "\n",
    "# from entity_gym.runner import CliRunner\n",
    "# from entity_gym.env import *\n",
    "\n",
    "# class TensorRTS(Environment):\n",
    "#     \"\"\"\n",
    "# LinearRTS, the first epoch of TensorRTS, is intended to be the simplest RTS game.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         mapsize: int = 32,\n",
    "#         nclusters: int = 6,\n",
    "#         ntensors: int = 2,\n",
    "#         maxdots: int = 9,\n",
    "#         enable_prinouts : bool = True\n",
    "#     ):\n",
    "#         self.enable_printouts = enable_prinouts\n",
    "        \n",
    "#         if self.enable_printouts:\n",
    "#             print(f\"LinearRTS -- Mapsize: {mapsize}\")\n",
    "\n",
    "#         self.mapsize = mapsize\n",
    "#         self.maxdots = maxdots\n",
    "#         self.nclusters = nclusters\n",
    "#         self.clusters: List[List[int]] = []  # The inner list has a size of 2 (position, number of dots).\n",
    "#         self.tensors: List[List[int ]] = [] # The inner list has a size of 4 (position, dimension, x, y).\n",
    "\n",
    "#     def obs_space(cls) -> ObsSpace:\n",
    "#         return ObsSpace(\n",
    "#             entities={\n",
    "#                 \"Cluster\": Entity(features=[\"position\", \"dot\"]),\n",
    "#                 \"Tensor\": Entity(features=[\"position\", \"dimension\", \"x\", \"y\"]),\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     def action_space(cls) -> Dict[ActionName, ActionSpace]:\n",
    "#         return {\n",
    "#             \"Move\": GlobalCategoricalActionSpace(\n",
    "#                 [\"advance\", \"retreat\", \"rush\", \"boom\"],\n",
    "#             ),\n",
    "#         }\n",
    "\n",
    "#     def reset(self) -> Observation:\n",
    "\n",
    "#         #I spent an out baning my head because turns out the reset doesnt delete the clusters from the last game\n",
    "#         self.clusters: List[List[int]] = []  # The inner list has a size of 2 (position, number of dots).\n",
    "#         self.tensors: List[List[int ]] = [] # The inner list has a size of 4 (position, dimension, x, y).\n",
    "#         positions = set()\n",
    "#         while len(positions) < self.nclusters // 2:\n",
    "#             position, b = random.choice(\n",
    "#                 [[position, b] for position in range(self.mapsize // 2) for b in range(1, self.maxdots)]\n",
    "#             )\n",
    "#             if position not in positions:\n",
    "#                 positions.add(position)\n",
    "#                 self.clusters.append([position, b])\n",
    "#                 self.clusters.append([self.mapsize - position - 1, b])\n",
    "#         self.clusters.sort()\n",
    " \n",
    "#         position = random.randint(0, self.mapsize // 2)\n",
    "#         self.tensors = [[position, 1, 2, 0], [self.mapsize - position - 1, 1, 2, 0]]\n",
    "\n",
    "#         if self.enable_printouts:\n",
    "#             self.print_universe()\n",
    "\n",
    "#         return self.observe()\n",
    "    \n",
    "#     def tensor_power(self, tensor_index) -> float :\n",
    "#         f = self.tensors[tensor_index][3] * self.tensors[tensor_index][3] +  self.tensors[tensor_index][2]\n",
    "#         if self.enable_printouts:\n",
    "#             print(f\"TP({tensor_index})=TP({self.tensors[tensor_index]})={f}\")\n",
    "#         return f\n",
    "\n",
    "#     def observe(self) -> Observation:\n",
    "#         done = self.tensors[0][0] >= self.tensors[1][0]\n",
    "#         if done:\n",
    "#             reward = 10 if self.tensor_power(0) > self.tensor_power(1) else 0 if self.tensor_power(0) == self.tensor_power(1) else -10\n",
    "#         else:\n",
    "#             reward = 1.0 if self.tensors[0][1] > self.tensors[1][1] else 0.0\n",
    "#         return Observation(\n",
    "#             entities={\n",
    "#                 \"Cluster\": (\n",
    "#                     self.clusters,\n",
    "#                     [(\"Cluster\", i) for i in range(len(self.clusters))],\n",
    "#                 ),\n",
    "#                 \"Tensor\": (\n",
    "#                     self.tensors,\n",
    "#                     [(\"Tensor\", i) for i in range(len(self.tensors))],\n",
    "#                 ),\n",
    "#             },\n",
    "#             actions={\n",
    "#                 \"Move\": GlobalCategoricalActionMask(),\n",
    "#             },\n",
    "#             done=done,\n",
    "#             reward=reward,\n",
    "#         )\n",
    "\n",
    "#     def act(self, actions: Mapping[ActionName, Action], trigger_default_opponent_action : bool = True, is_player_two : bool = False) -> Observation:\n",
    "#         action = actions[\"Move\"]\n",
    "\n",
    "#         player_tensor = self.tensors[0]\n",
    "#         if is_player_two:\n",
    "#             player_tensor = self.tensors[1]\n",
    "\n",
    "#         assert isinstance(action, GlobalCategoricalAction)\n",
    "#         if action.label == \"advance\" and self.tensors[0][0] < self.mapsize:\n",
    "#             player_tensor[0] += 1\n",
    "#             player_tensor[2] += self.collect_dots(player_tensor[0])\n",
    "#         elif action.label == \"retreat\" and player_tensor[0] > 0:\n",
    "#             player_tensor[0] -= 1\n",
    "#             player_tensor[2] += self.collect_dots(player_tensor[0])\n",
    "#         elif action.label == \"boom\":\n",
    "#             player_tensor[2] += 1\n",
    "#         elif action.label == \"rush\":\n",
    "#             if player_tensor[2] >= 1:\n",
    "#                 player_tensor[1] = 2 # the number of dimensions is now 2\n",
    "#                 player_tensor[2] -= 1\n",
    "#                 player_tensor[3] += 1\n",
    "\n",
    "#         if trigger_default_opponent_action:\n",
    "#             self.opponent_act()\n",
    "        \n",
    "#         if self.enable_printouts:\n",
    "#             self.print_universe()\n",
    "\n",
    "#         return self.observe()\n",
    "\n",
    "#     def opponent_act(self):         # This is the rush AI.\n",
    "#         if self.tensors[1][2]>0 :   # Rush if possile\n",
    "#             self.tensors[1][2] -= 1\n",
    "#             self.tensors[1][3] += 1\n",
    "#             self.tensors[1][1] = 2      # the number of dimensions is now 2\n",
    "#         else:                       # Otherwise Advance.\n",
    "#             self.tensors[1][0] -= 1\n",
    "#             self.tensors[1][2] += self.collect_dots(self.tensors[1][0])\n",
    "\n",
    "#         return self.observe()\n",
    "\n",
    "#     def collect_dots(self, position):\n",
    "#         low, high = 0, len(self.clusters) - 1\n",
    "\n",
    "#         while low <= high:\n",
    "#             mid = (low + high) // 2\n",
    "#             current_value = self.clusters[mid][0]\n",
    "\n",
    "#             if current_value == position:\n",
    "#                 dots = self.clusters[mid][1]\n",
    "#                 self.clusters[mid][1] = 0\n",
    "#                 return dots\n",
    "#             elif current_value < position:\n",
    "#                 low = mid + 1\n",
    "#             else:\n",
    "#                 high = mid - 1\n",
    "\n",
    "#         return 0        \n",
    "\n",
    "#     def print_universe(self):\n",
    "#         #    print(self.clusters)\n",
    "#         #    print(self.tensors)\n",
    "#         for j in range(self.mapsize):\n",
    "#             print(f\" {j%10}\", end=\"\")\n",
    "#         print(\" #\")\n",
    "#         position_init = 0\n",
    "#         for i in range(len(self.clusters)):\n",
    "#             for j in range(position_init, self.clusters[i][0]):\n",
    "#                 print(\"  \", end=\"\")\n",
    "#             print(f\" {self.clusters[i][1]}\", end=\"\")\n",
    "#             position_init = self.clusters[i][0]+1\n",
    "#         for j in range(position_init, self.mapsize):\n",
    "#             print(\"  \", end=\"\")\n",
    "#         print(\" ##\")\n",
    "\n",
    "#         position_init = 0\n",
    "#         for i in range(len(self.tensors)):\n",
    "#             for j in range(position_init, self.tensors[i][0]):\n",
    "#                 print(\"  \", end=\"\")\n",
    "#             print(f\"{self.tensors[i][2]}\", end=\"\")\n",
    "#             if self.tensors[i][3]>=0:\n",
    "#                 print(f\"-{self.tensors[i][3]}\", end=\"\")\n",
    "#             position_init = self.tensors[i][0]+1\n",
    "#         for j in range(position_init, self.mapsize):\n",
    "#             print(\"  \", end=\"\")\n",
    "#         print(\" ##\")\n",
    "\n",
    "# class Interactive_TensorRTS(TensorRTS): \n",
    "#     def __init__(self,\n",
    "#         mapsize: int = 32,\n",
    "#         nclusters: int = 6,\n",
    "#         ntensors: int = 2,\n",
    "#         maxdots: int = 9, \n",
    "#         enable_printouts : bool = True): \n",
    "#         self.is_game_over = False\n",
    "\n",
    "#         super().__init__(mapsize, nclusters, ntensors, maxdots, enable_prinouts=enable_printouts)\n",
    "\n",
    "#     def act(self, actions: Mapping[ActionName, Action],  trigger_default_opponent_action : bool = True, is_player_two : bool = False, print_universe : bool = False) -> Observation:\n",
    "#         obs_result = super().act(actions, False, is_player_two)\n",
    "\n",
    "#         if (obs_result.done == True):\n",
    "#             self.is_game_over = True\n",
    "\n",
    "#         return obs_result\n",
    "\n",
    "# class Agent(metaclass=abc.ABCMeta):\n",
    "#     def __init__(self, initial_observation : Observation, action_space : Dict[ActionName, ActionSpace]):\n",
    "#         self.previous_game_state = initial_observation\n",
    "#         self.action_space = action_space\n",
    "\n",
    "#     @abc.abstractmethod\n",
    "#     def take_turn(self, current_game_state : Observation) -> Mapping[ActionName, Action]: \n",
    "#         \"\"\"Pure virtual function in which an agent should return the move that they will make on this turn.\n",
    "\n",
    "#         Returns:\n",
    "#             str: name of the action that will be taken\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "\n",
    "#     @abc.abstractmethod\n",
    "#     def on_game_start(self) -> None: \n",
    "#         \"\"\"Function which is called for the agent before the game begins.\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "\n",
    "#     @abc.abstractmethod\n",
    "#     def on_game_over(self, did_i_win : bool, did_i_tie : bool) -> None:\n",
    "#         \"\"\"Function which is called for the agent once the game is over.\n",
    "\n",
    "#         Args:\n",
    "#             did_i_win (bool): set to True if this agent won the game.\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "\n",
    "# class Random_Agent(Agent):\n",
    "#     def __init__(self, init_observation : Observation, action_space : Dict[ActionName, ActionSpace]) -> None: \n",
    "#         super().__init__(init_observation, action_space)\n",
    "\n",
    "#     def take_turn(self, current_game_state : Observation) -> Mapping[ActionName, Action]:\n",
    "#         mapping = {}\n",
    "\n",
    "#         action_choice = random.randrange(0, 2)\n",
    "#         if (action_choice == 1): \n",
    "#             mapping['Move'] = GlobalCategoricalAction(0, self.action_space['Move'].index_to_label[0])\n",
    "#         else: \n",
    "#             mapping[\"Move\"] = GlobalCategoricalAction(1, self.action_space['Move'].index_to_label[1])\n",
    "        \n",
    "#         return mapping\n",
    "    \n",
    "#     def on_game_start(self) -> None:\n",
    "#         return super().on_game_start()\n",
    "    \n",
    "#     def on_game_over(self, did_i_win : bool, did_i_tie : bool) -> None:\n",
    "#         return super().on_game_over(did_i_win, did_i_tie)\n",
    "\n",
    "# class GameResult():\n",
    "#     def __init__(self, player_one_win : bool = False, player_two_win : bool = False, tie : bool = False):\n",
    "#         self.player_one_win = player_one_win\n",
    "#         self.player_two_win = player_two_win\n",
    "#         self.tie = tie\n",
    "\n",
    "# class GameRunner(): \n",
    "#     def __init__(self, environment = None, enable_printouts : bool = False):\n",
    "#         self.game = Interactive_TensorRTS(enable_printouts=enable_printouts)\n",
    "#         self.game.reset()\n",
    "\n",
    "#         self.player_one = None\n",
    "#         self.player_two = None\n",
    "#         self.results : GameResult = None\n",
    "    \n",
    "#     def assign_players(self, first_agent : Agent, second_agent : Agent = None):\n",
    "#         self.player_one = first_agent\n",
    "\n",
    "#         if second_agent is not None:\n",
    "#             self.player_two = second_agent\n",
    "\n",
    "#     def run(self): \n",
    "#         assert(self.player_one is not None)\n",
    "\n",
    "#         game_state = self.game.observe()\n",
    "#         self.player_one.on_game_start()\n",
    "#         if self.player_two is not None: \n",
    "#             self.player_two.on_game_start()\n",
    "\n",
    "#         while(self.game.is_game_over is False):\n",
    "#             #take moves and pass updated environments to agents\n",
    "#             game_state = self.game.act(self.player_one.take_turn(game_state))\n",
    "            \n",
    "#             if (self.game.is_game_over is False):\n",
    "#                 if self.player_two is None: \n",
    "#                     game_state = self.game.opponent_act()\n",
    "#                 else:\n",
    "#                     #future player_two code\n",
    "#                     game_state = self.game.act(self.player_two.take_turn(game_state), False, True)\n",
    "\n",
    "#         # who won? \n",
    "#         tie = False\n",
    "#         win_p_one = False\n",
    "#         win_p_two = False\n",
    "\n",
    "#         p_one = self.game.tensor_power(0)\n",
    "#         p_two = self.game.tensor_power(1)\n",
    "\n",
    "#         if p_one > p_two: \n",
    "#             win_p_one = True\n",
    "#         elif p_two > p_one:\n",
    "#             win_p_two = True\n",
    "#         else:\n",
    "#             tie = True\n",
    "\n",
    "#         self.results = GameResult(win_p_one, win_p_two, tie)\n",
    "#         self.player_one.on_game_over(win_p_one, tie)\n",
    "#         if self.player_two is not None:\n",
    "#             self.player_two.on_game_over(win_p_two, tie)\n",
    "\n",
    "# # if __name__ == \"__main__\":  # This is to run wth agents\n",
    "# #     runner = GameRunner()\n",
    "# #     init_observation = runner.set_new_game()\n",
    "# #     random_agent = Random_Agent(init_observation, runner.game.action_space())\n",
    "\n",
    "# #     runner.assign_players(random_agent)\n",
    "# #     runner.run()\n",
    "    \n",
    "# # if __name__ == \"__main__\":  #this is to run cli\n",
    "# #     env = TensorRTS()\n",
    "# #     # The `CliRunner` can run any environment with a command line interface.\n",
    "# #     CliRunner(env).run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# import numpy as np\n",
    "# from gymnasium import spaces\n",
    "\n",
    "\n",
    "# class CustomEnv(gym.Env):\n",
    "#     \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "#     metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "\n",
    "#         self.tensor = TensorRTS();\n",
    "#         # Define action and observation space\n",
    "#         # They must be gym.spaces objects\n",
    "#         # Example when using discrete actions:\n",
    "#         self.action_space = spaces.utils.flatten_space(spaces.Discrete(4))\n",
    "#         # Example for using image as input (channel-first; channel-last also works):\n",
    "#         # self.observation_space = spaces.Box(low=0, high=255,\n",
    "#         #                                     shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n",
    "    \n",
    "        \n",
    "\n",
    "#         self.observation_space_unflatten = spaces.Dict({\n",
    "#              \"Cluster\": spaces.Box(low=0, high=self.tensor.maxdots, shape=(self.tensor.nclusters, 2),dtype=int),\n",
    "#              \"Tensor\": spaces.Box(low=0, high=100, shape=(2, 4), dtype=int)\n",
    "#             }\n",
    "#             )\n",
    "#         self.observation_space = spaces.utils.flatten_space(self.observation_space_unflatten)\n",
    "\n",
    "#     def step(self, action):\n",
    "#         ...\n",
    "#         return observation, reward, terminated, truncated, info\n",
    "\n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         super().reset()\n",
    "#         self.tensor.reset()\n",
    "#         return self._get_obs()\n",
    "\n",
    "#     def render(self):\n",
    "#         ...\n",
    "\n",
    "#     def close(self):\n",
    "#         ...\n",
    "\n",
    "\n",
    "#     def _get_obs(self):\n",
    "#             return {\"Cluster\": self.tensor.clusters, \"Tensor\": self.tensor.tensors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thank you ai\n",
    "import functools\n",
    "def flatten_dict_of_arrays(data, prefix=\"\"):\n",
    "  flattened_list = []\n",
    "  def flatten_inner(value, key):\n",
    "    if isinstance(value, list):\n",
    "      for i, v in enumerate(value):\n",
    "        flatten_inner(v, f\"{prefix}{key}[{i}]\")\n",
    "    else:\n",
    "      flattened_list.append((f\"{prefix}{key}\", value))\n",
    "\n",
    "  for key, value in data.items():\n",
    "    flatten_inner(value, key)\n",
    "\n",
    "  return [value for _, value in flattened_list]\n",
    "\n",
    "\n",
    "def value_to_discrete_4(input):\n",
    "    output = [0,0,0,0]\n",
    "    output[input] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This generates game state data using a random agent vs rush agent\n",
    "from entity_gym.env import * #for GlobalCategoricalAction\n",
    "\n",
    "possible_moves = {\n",
    "    0 : \"advance\",\n",
    "    1 : \"retreat\",\n",
    "    2 : \"rush\",\n",
    "    3 : \"boom\"\n",
    "}\n",
    "rdmEnv = TensorRTS(enable_prinouts = False)\n",
    "rdmEnv.reset()\n",
    "\n",
    "rndDataset = {\n",
    "    'observations': [],\n",
    "    'actions': [], \n",
    "    'rewards': [], \n",
    "    'dones': [],\n",
    "    \n",
    "}\n",
    "\n",
    "action = {}\n",
    "for _ in range(100):\n",
    "    rdmEnv.reset()\n",
    "\n",
    "    o = []\n",
    "    a = []\n",
    "    r = []\n",
    "    d = []\n",
    "    \n",
    "    for x in range(100):\n",
    "        o.append(flatten_dict_of_arrays(rdmEnv.observe().features))\n",
    "        move = random.randrange(0, 4)\n",
    "        action[\"Move\"] = GlobalCategoricalAction(0, possible_moves[move])\n",
    "        a.append(value_to_discrete_4(move))\n",
    "\n",
    "        model_out = rdmEnv.act(action)\n",
    "\n",
    "        r.append(model_out.reward)\n",
    "\n",
    "        d.append(model_out.done)\n",
    "        if model_out.done == True: \n",
    "            break \n",
    "\n",
    "    rndDataset['observations'].append(o)\n",
    "    rndDataset['actions'].append(a)\n",
    "    rndDataset['rewards'].append(r)\n",
    "    rndDataset['dones'].append(d)\n",
    "    # rndList.append(rndDataset)\n",
    "\n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_dict(rndDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[1][\"observations\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecisionTransformerGymDataCollator:\n",
    "#     return_tensors: str = \"pt\"\n",
    "#     max_len: int = 20 #subsets of the episode we use for training\n",
    "#     state_dim: int = 20  # size of state space\n",
    "#     act_dim: int = 4  # size of action space\n",
    "#     max_ep_len: int = 1000 # max episode length in the dataset\n",
    "#     scale: float = 1000.0  # normalization of rewards/returns\n",
    "#     state_mean: np.array = None  # to store state means\n",
    "#     state_std: np.array = None  # to store state stds\n",
    "#     p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "#     n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "#     def __init__(self, dataset) -> None:\n",
    "#         self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "#         self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "#         self.dataset = dataset\n",
    "#         # calculate dataset stats for normalization of states\n",
    "#         states = []\n",
    "#         traj_lens = []\n",
    "#         for obs in dataset[\"observations\"]:\n",
    "#             states.extend(obs)\n",
    "#             traj_lens.append(len(obs))\n",
    "#         self.n_traj = len(traj_lens)\n",
    "#         states = np.vstack(states)\n",
    "#         self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "#         traj_lens = np.array(traj_lens)\n",
    "#         self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "#     def _discount_cumsum(self, x, gamma):\n",
    "#         discount_cumsum = np.zeros_like(x)\n",
    "#         discount_cumsum[-1] = x[-1]\n",
    "#         for t in reversed(range(x.shape[0] - 1)):\n",
    "#             discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "#         return discount_cumsum\n",
    "\n",
    "#     def __call__(self, features):\n",
    "#         batch_size = len(features)\n",
    "#         # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "#         batch_inds = np.random.choice(\n",
    "#             np.arange(self.n_traj),\n",
    "#             size=batch_size,\n",
    "#             replace=True,\n",
    "#             p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "#         )\n",
    "#         # a batch of dataset features\n",
    "#         s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "#         for ind in batch_inds:\n",
    "#             # for feature in features:\n",
    "#             feature = self.dataset[int(ind)]\n",
    "#             si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "#             # get sequences from dataset\n",
    "#             s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "#             a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "#             r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "#             d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "#             timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "#             timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "#             rtg.append(\n",
    "#                 self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "#                     : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "#                 ].reshape(1, -1, 1)\n",
    "#             )\n",
    "#             if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "#                 print(\"if true\")\n",
    "#                 rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "#             # padding and state + reward normalization\n",
    "#             tlen = s[-1].shape[1]\n",
    "#             s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "#             s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "#             a[-1] = np.concatenate(\n",
    "#                 [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "#                 axis=1,\n",
    "#             )\n",
    "#             r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "#             d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "#             rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "#             timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "#             mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "#         s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "#         a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "#         r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "#         d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "#         rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "#         timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "#         mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "#         return {\n",
    "#             \"states\": s,\n",
    "#             \"actions\": a,\n",
    "#             \"rewards\": r,\n",
    "#             \"returns_to_go\": rtg,\n",
    "#             \"timesteps\": timesteps,\n",
    "#             \"attention_mask\": mask,\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainableDT(DecisionTransformerModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#     def forward(self, **kwargs):\n",
    "#         output = super().forward(**kwargs)\n",
    "#         # add the DT loss\n",
    "#         action_preds = output[1]\n",
    "#         action_targets = kwargs[\"actions\"]\n",
    "#         attention_mask = kwargs[\"attention_mask\"]\n",
    "#         act_dim = action_preds.shape[2]\n",
    "#         action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "#         action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "#         loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "\n",
    "#         return {\"loss\": loss}\n",
    "\n",
    "#     def original_forward(self, **kwargs):\n",
    "#         return super().forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DTscripts\n",
    "\n",
    "collator = DTscripts.DecisionTransformerGymDataCollator(ds)\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = DTscripts.TrainableDT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed79b89677354d54bffc5c3e79f140fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5.6067, 'train_samples_per_second': 356.717, 'train_steps_per_second': 7.134, 'train_loss': 0.18081194162368774, 'epoch': 20.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DTscripts.TrainableDT.from_pretrained(\"./output/models4\")\n",
    "    # output_dir=\"output/\",\n",
    "    # remove_unused_columns=False,\n",
    "    # num_train_epochs=120,\n",
    "    # per_device_train_batch_size=64,\n",
    "    # learning_rate=1e-4,\n",
    "    # weight_decay=1e-4,\n",
    "    # warmup_ratio=0.1,\n",
    "    # optim=\"adamw_torch\",\n",
    "    # max_grad_norm=0.25,\n",
    "import wandb\n",
    "# wandb.init(project=\"VectorRTS-transformer\")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\" \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "    use_mps_device=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps = 10\n",
    "\n",
    ")\n",
    "training_args = training_args.set_save(strategy=\"steps\", steps=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./output/models4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainableDT(\n",
       "  (encoder): DecisionTransformerGPT2Model(\n",
       "    (wte): Embedding(1, 128)\n",
       "    (wpe): Embedding(1024, 128)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-2): 3 x DecisionTransformerGPT2Block(\n",
       "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): DecisionTransformerGPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): DecisionTransformerGPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_timestep): Embedding(4096, 128)\n",
       "  (embed_return): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (embed_state): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (embed_action): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (embed_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (predict_state): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (predict_action): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=4, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (predict_return): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stolen\n",
    "# def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "#     # This implementation does not condition on past rewards\n",
    "\n",
    "#     states = states.reshape(1, -1, model.config.state_dim)\n",
    "#     actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "#     returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "#     timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "#     states = states[:, -model.config.max_length :]\n",
    "#     actions = actions[:, -model.config.max_length :]\n",
    "#     returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "#     timesteps = timesteps[:, -model.config.max_length :]\n",
    "#     padding = model.config.max_length - states.shape[1]\n",
    "#     # pad all tokens to sequence length\n",
    "#     attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "#     attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "#     states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "#     actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "#     returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "#     timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "#     state_preds, action_preds, return_preds = model.original_forward(\n",
    "#         states=states,\n",
    "#         actions=actions,\n",
    "#         rewards=rewards,\n",
    "#         returns_to_go=returns_to_go,\n",
    "#         timesteps=timesteps,\n",
    "#         attention_mask=attention_mask,\n",
    "#         return_dict=False,\n",
    "#     )\n",
    "\n",
    "#     return action_preds[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_move_no_lookback(state):\n",
    "    state_dim = 20\n",
    "    act_dim = 4\n",
    "    states = torch.from_numpy(state).reshape(1, 1, state_dim).to(device=device, dtype=torch.float32)\n",
    "    actions = torch.zeros((1, 1, act_dim), device=device, dtype=torch.float32)\n",
    "    rewards = torch.zeros(1, 1, device=device, dtype=torch.float32)\n",
    "    target_return = torch.tensor(10, dtype=torch.float32).reshape(1, 1)\n",
    "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "    attention_mask = torch.zeros(1, 1, device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        state_preds, action_preds, return_preds = model(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            returns_to_go=target_return,\n",
    "            timesteps=timesteps,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False,\n",
    "            # use_mps_device=True\n",
    "\n",
    "        )\n",
    "    return state_preds, action_preds, return_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from entity_gym.env import Observation\n",
    "\n",
    "from entity_gym.env import *\n",
    "def evaluate_model(model,num_runs):\n",
    "    testEnv = TensorRTS(enable_prinouts = False)\n",
    "    possible_moves = {\n",
    "        0 : \"advance\",\n",
    "        1 : \"retreat\",\n",
    "        2 : \"rush\",\n",
    "        3 : \"boom\"\n",
    "    }\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    ties = 0\n",
    "    for _ in range(num_runs):\n",
    "\n",
    "        testEnv.reset()\n",
    "        for _ in range(1000):\n",
    "            #make prediction\n",
    "            model_out = get_move_no_lookback(np.array(flatten_dict_of_arrays(testEnv.observe().features)))\n",
    "            action['Move'] = GlobalCategoricalAction(0, possible_moves[torch.argmax(model_out[1]).item()])\n",
    "            pred_reward = model_out[2]\n",
    "            result = testEnv.act(action)\n",
    "            # print(model_out[1])\n",
    "\n",
    "            if result.done == True: \n",
    "                break \n",
    "            \n",
    "\n",
    "        # print(result.reward)\n",
    "        if (result.reward == 10): \n",
    "            wins+=1\n",
    "            print(wins)\n",
    "        if (result.reward == 0): ties +=1\n",
    "        if (result.reward == -10): losses+=1\n",
    "\n",
    "\n",
    "    return ((wins+ties)/num_runs)\n",
    "model = DecisionTransformerModel.from_pretrained(\"./output/models4\")\n",
    "\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_move_no_lookback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m testEnv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#make prediction\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[43mget_move_no_lookback\u001b[49m(np\u001b[38;5;241m.\u001b[39marray(flatten_dict_of_arrays(testEnv\u001b[38;5;241m.\u001b[39mobserve()\u001b[38;5;241m.\u001b[39mfeatures)))\n\u001b[1;32m     24\u001b[0m     action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMove\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m GlobalCategoricalAction(\u001b[38;5;241m0\u001b[39m, possible_moves[torch\u001b[38;5;241m.\u001b[39margmax(model_out[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()])\n\u001b[1;32m     25\u001b[0m     pred_reward \u001b[38;5;241m=\u001b[39m model_out[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_move_no_lookback' is not defined"
     ]
    }
   ],
   "source": [
    "model = DecisionTransformerModel.from_pretrained(\"./output/models3\")\n",
    "# evaluation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "possible_moves = {\n",
    "    0 : \"advance\",\n",
    "    1 : \"retreat\",\n",
    "    2 : \"rush\",\n",
    "    3 : \"boom\"\n",
    "}\n",
    "action = {}\n",
    "\n",
    "\n",
    "testEnv = TensorRTS(enable_prinouts = False)\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    testEnv.reset()\n",
    "    for _ in range(100):\n",
    "        #make prediction\n",
    "        model_out = get_move_no_lookback(np.array(flatten_dict_of_arrays(testEnv.observe().features)))\n",
    "        action['Move'] = GlobalCategoricalAction(0, possible_moves[torch.argmax(model_out[1]).item()])\n",
    "        pred_reward = model_out[2]\n",
    "        result = testEnv.act(action)\n",
    "        # print(model_out[1])\n",
    "\n",
    "        if result.done == True: \n",
    "            break \n",
    "\n",
    "    total_reward += result.reward\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "total_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
