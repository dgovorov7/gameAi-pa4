{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempts at training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### some resources I've been using\n",
    "huggingface decision transformer:https://huggingface.co/docs/transformers/en/model_doc/decision_transformer\n",
    "\n",
    "training huggingface decision transformer: https://github.com/huggingface/blog/blob/main/notebooks/101_train-decision-transformers.ipynb\n",
    "\n",
    "\n",
    "custom gym environments:https://www.gymlibrary.dev/content/environment_creation/\n",
    "\n",
    "gymnasium documentation: https://gymnasium.farama.org/\n",
    "\n",
    "decision transformer github: https://github.com/kzl/decision-transformer/tree/master\n",
    "\n",
    "very simple DT: https://github.com/nikhilbarhate99/min-decision-transformer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "\n",
    "from TensorRTS import TensorRTS\n",
    "\n",
    "from entity_gym.env import Observation\n",
    "from entity_gym.env import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# import tensorRTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gymnasium wrapper for tensorRTS; I did not finish this but it may be useful in the future\n",
    "\n",
    "# import gymnasium as gym\n",
    "# import numpy as np\n",
    "# from gymnasium import spaces\n",
    "\n",
    "\n",
    "# class CustomEnv(gym.Env):\n",
    "#     \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "#     metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "\n",
    "#         self.tensor = TensorRTS();\n",
    "#         # Define action and observation space\n",
    "#         # They must be gym.spaces objects\n",
    "#         # Example when using discrete actions:\n",
    "#         self.action_space = spaces.utils.flatten_space(spaces.Discrete(4))\n",
    "#         # Example for using image as input (channel-first; channel-last also works):\n",
    "#         # self.observation_space = spaces.Box(low=0, high=255,\n",
    "#         #                                     shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n",
    "    \n",
    "        \n",
    "\n",
    "#         self.observation_space_unflatten = spaces.Dict({\n",
    "#              \"Cluster\": spaces.Box(low=0, high=self.tensor.maxdots, shape=(self.tensor.nclusters, 2),dtype=int),\n",
    "#              \"Tensor\": spaces.Box(low=0, high=100, shape=(2, 4), dtype=int)\n",
    "#             }\n",
    "#             )\n",
    "#         self.observation_space = spaces.utils.flatten_space(self.observation_space_unflatten)\n",
    "\n",
    "#     def step(self, action):\n",
    "#         ...\n",
    "#         return observation, reward, terminated, truncated, info\n",
    "\n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         super().reset()\n",
    "#         self.tensor.reset()\n",
    "#         return self._get_obs()\n",
    "\n",
    "#     def render(self):\n",
    "#         ...\n",
    "\n",
    "#     def close(self):\n",
    "#         ...\n",
    "\n",
    "\n",
    "#     def _get_obs(self):\n",
    "#             return {\"Cluster\": self.tensor.clusters, \"Tensor\": self.tensor.tensors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thank you ai\n",
    "import functools\n",
    "def flatten_dict_of_arrays(data, prefix=\"\"):\n",
    "  flattened_list = []\n",
    "  def flatten_inner(value, key):\n",
    "    if isinstance(value, list):\n",
    "      for i, v in enumerate(value):\n",
    "        flatten_inner(v, f\"{prefix}{key}[{i}]\")\n",
    "    else:\n",
    "      flattened_list.append((f\"{prefix}{key}\", value))\n",
    "\n",
    "  for key, value in data.items():\n",
    "    flatten_inner(value, key)\n",
    "\n",
    "  return [value for _, value in flattened_list]\n",
    "\n",
    "\n",
    "def value_to_discrete_4(input):\n",
    "    output = [0,0,0,0]\n",
    "    output[input] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This generates game state data using a random agent vs rush agent\n",
    "from entity_gym.env import * #for GlobalCategoricalAction\n",
    "possible_moves = {\n",
    "    0 : \"advance\",\n",
    "    1 : \"retreat\",\n",
    "    2 : \"rush\",\n",
    "    3 : \"boom\"\n",
    "}\n",
    "rdmEnv = TensorRTS(enable_prinouts = False)\n",
    "rdmEnv.reset()\n",
    "rndDataset = {\n",
    "    'observations': [],\n",
    "    'actions': [], \n",
    "    'rewards': [], \n",
    "    'dones': [],\n",
    "    \n",
    "}\n",
    "action = {}\n",
    "for _ in range(100):\n",
    "    rdmEnv.reset()\n",
    "\n",
    "    o = []\n",
    "    a = []\n",
    "    r = []\n",
    "    d = []\n",
    "    \n",
    "    for x in range(100):\n",
    "        o.append(flatten_dict_of_arrays(rdmEnv.observe().features))\n",
    "        move = random.randrange(0, 4)\n",
    "        action[\"Move\"] = GlobalCategoricalAction(0, possible_moves[move])\n",
    "        a.append(value_to_discrete_4(move))\n",
    "\n",
    "        model_out = rdmEnv.act(action)\n",
    "\n",
    "        r.append(model_out.reward)\n",
    "\n",
    "        d.append(model_out.done)\n",
    "        if model_out.done == True: \n",
    "            break \n",
    "\n",
    "    rndDataset['observations'].append(o)\n",
    "    rndDataset['actions'].append(a)\n",
    "    rndDataset['rewards'].append(r)\n",
    "    rndDataset['dones'].append(d)\n",
    "    # rndList.append(rndDataset)\n",
    "\n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_dict(rndDataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in base model for training and collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DTscripts\n",
    "\n",
    "collator = DTscripts.DecisionTransformerGymDataCollator(ds)\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = DTscripts.TrainableDT(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdgovorov7\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/den/Documents/CS/classwork/4900/gameAi-pa4/wandb/run-20240307_144639-o7jtkwb2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dgovorov7/huggingface/runs/o7jtkwb2' target=\"_blank\">exalted-firefly-4</a></strong> to <a href='https://wandb.ai/dgovorov7/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dgovorov7/huggingface' target=\"_blank\">https://wandb.ai/dgovorov7/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dgovorov7/huggingface/runs/o7jtkwb2' target=\"_blank\">https://wandb.ai/dgovorov7/huggingface/runs/o7jtkwb2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db58bedfaca644b8ab867ea810382c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 9.2059, 'train_samples_per_second': 217.251, 'train_steps_per_second': 4.345, 'train_loss': 0.21411147117614746, 'epoch': 20.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = DTscripts.TrainableDT.from_pretrained(\"./output/models4\")\n",
    "import wandb\n",
    "# wandb.init(project=\"VectorRTS-transformer\")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "    use_mps_device=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps = 10\n",
    "\n",
    ")\n",
    "training_args = training_args.set_save(strategy=\"steps\", steps=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./output/models20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stolen\n",
    "# def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "#     # This implementation does not condition on past rewards\n",
    "\n",
    "#     states = states.reshape(1, -1, model.config.state_dim)\n",
    "#     actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "#     returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "#     timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "#     states = states[:, -model.config.max_length :]\n",
    "#     actions = actions[:, -model.config.max_length :]\n",
    "#     returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "#     timesteps = timesteps[:, -model.config.max_length :]\n",
    "#     padding = model.config.max_length - states.shape[1]\n",
    "#     # pad all tokens to sequence length\n",
    "#     attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "#     attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "#     states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "#     actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "#     returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "#     timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "#     state_preds, action_preds, return_preds = model.original_forward(\n",
    "#         states=states,\n",
    "#         actions=actions,\n",
    "#         rewards=rewards,\n",
    "#         returns_to_go=returns_to_go,\n",
    "#         timesteps=timesteps,\n",
    "#         attention_mask=attention_mask,\n",
    "#         return_dict=False,\n",
    "#     )\n",
    "\n",
    "#     return action_preds[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_move_no_lookback(state):\n",
    "    state_dim = 20\n",
    "    act_dim = 4\n",
    "    states = torch.from_numpy(state).reshape(1, 1, state_dim).to(device=device, dtype=torch.float32)\n",
    "    actions = torch.zeros((1, 1, act_dim), device=device, dtype=torch.float32)\n",
    "    rewards = torch.zeros(1, 1, device=device, dtype=torch.float32)\n",
    "    target_return = torch.tensor(10, dtype=torch.float32).reshape(1, 1)\n",
    "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "    attention_mask = torch.zeros(1, 1, device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        state_preds, action_preds, return_preds = model(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            returns_to_go=target_return,\n",
    "            timesteps=timesteps,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False,\n",
    "            # use_mps_device=True\n",
    "\n",
    "        )\n",
    "    return state_preds, action_preds, return_preds\n",
    "\n",
    "def evaluate_model(model,num_runs):\n",
    "    testEnv = TensorRTS(enable_prinouts = False)\n",
    "    possible_moves = {\n",
    "        0 : \"advance\",\n",
    "        1 : \"retreat\",\n",
    "        2 : \"rush\",\n",
    "        3 : \"boom\"\n",
    "    }\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    ties = 0\n",
    "    for _ in range(num_runs):\n",
    "\n",
    "        testEnv.reset()\n",
    "        for _ in range(1000):\n",
    "            #make prediction\n",
    "            model_out = get_move_no_lookback(np.array(flatten_dict_of_arrays(testEnv.observe().features)))\n",
    "            action['Move'] = GlobalCategoricalAction(0, possible_moves[torch.argmax(model_out[1]).item()])\n",
    "            pred_reward = model_out[2]\n",
    "            result = testEnv.act(action)\n",
    "            # print(model_out[1])\n",
    "\n",
    "            if result.done == True: \n",
    "                break \n",
    "            \n",
    "\n",
    "        # print(result.reward)\n",
    "        if (result.reward == 10): \n",
    "            wins+=1\n",
    "            print(wins)\n",
    "        if (result.reward == 0): ties +=1\n",
    "        if (result.reward == -10): losses+=1\n",
    "\n",
    "\n",
    "    return ((wins+ties)/num_runs)\n",
    "model = DecisionTransformerModel.from_pretrained(\"./output/models4\")\n",
    "\n",
    "evaluate_model(model,100) #Returns win rate + tie rate in %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_move_no_lookback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m testEnv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#make prediction\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[43mget_move_no_lookback\u001b[49m(np\u001b[38;5;241m.\u001b[39marray(flatten_dict_of_arrays(testEnv\u001b[38;5;241m.\u001b[39mobserve()\u001b[38;5;241m.\u001b[39mfeatures)))\n\u001b[1;32m     24\u001b[0m     action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMove\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m GlobalCategoricalAction(\u001b[38;5;241m0\u001b[39m, possible_moves[torch\u001b[38;5;241m.\u001b[39margmax(model_out[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()])\n\u001b[1;32m     25\u001b[0m     pred_reward \u001b[38;5;241m=\u001b[39m model_out[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_move_no_lookback' is not defined"
     ]
    }
   ],
   "source": [
    "# model = DecisionTransformerModel.from_pretrained(\"./output/models3\")\n",
    "# # evaluation\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "# possible_moves = {\n",
    "#     0 : \"advance\",\n",
    "#     1 : \"retreat\",\n",
    "#     2 : \"rush\",\n",
    "#     3 : \"boom\"\n",
    "# }\n",
    "# action = {}\n",
    "\n",
    "\n",
    "# testEnv = TensorRTS(enable_prinouts = False)\n",
    "\n",
    "# total_reward = 0\n",
    "\n",
    "# for _ in range(10):\n",
    "\n",
    "#     testEnv.reset()\n",
    "#     for _ in range(100):\n",
    "#         #make prediction\n",
    "#         model_out = get_move_no_lookback(np.array(flatten_dict_of_arrays(testEnv.observe().features)))\n",
    "#         action['Move'] = GlobalCategoricalAction(0, possible_moves[torch.argmax(model_out[1]).item()])\n",
    "#         pred_reward = model_out[2]\n",
    "#         result = testEnv.act(action)\n",
    "#         # print(model_out[1])\n",
    "\n",
    "#         if result.done == True: \n",
    "#             break \n",
    "\n",
    "#     total_reward += result.reward\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# total_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
