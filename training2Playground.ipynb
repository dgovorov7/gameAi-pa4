{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second attempt at training\n",
    "This time im trying to set up an environments that would allow me to have a variety of bots for generating the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TensorRTS import Agent, GameResult, Interactive_TensorRTS\n",
    "\n",
    "from tournament_runner import Bot, Matchup, Bracket, Tournament\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "# tensor_path = os.path.abspath(os.path.join(os.path.basename(__file__), os.pardir, os.pardir, os.pardir))\n",
    "# sys.path.append(tensor_path)\n",
    "\n",
    "import random \n",
    "from typing import Dict, List, Mapping, Tuple, Set\n",
    "from entity_gym.env import Observation\n",
    "from entity_gym.runner import CliRunner\n",
    "from entity_gym.env import *\n",
    "from TensorRTS import Agent\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rush agent on the left side\n",
    "class Rush_Agent_Player_2(Agent):\n",
    "    def __init__(self, init_observation : Observation, action_space : Dict[ActionName, ActionSpace]) -> None: \n",
    "        super().__init__(init_observation, action_space)\n",
    "\n",
    "    def take_turn(self, current_game_state : Observation) -> Mapping[ActionName, Action]:\n",
    "        mapping = {}\n",
    "        if current_game_state.features['Tensor'][1][2] > 0 :\n",
    "            #rush\n",
    "            mapping['Move'] = GlobalCategoricalAction(0, self.action_space['Move'].index_to_label[1])\n",
    "        else:\n",
    "        #     #advance\n",
    "            mapping[\"Move\"] = GlobalCategoricalAction(1, self.action_space['Move'].index_to_label[2])\n",
    "        return mapping\n",
    "    \n",
    "    def on_game_start(self) -> None:\n",
    "        return super().on_game_start()\n",
    "    \n",
    "    def on_game_over(self, did_i_win : bool, did_i_tie : bool) -> None:\n",
    "        return super().on_game_over(did_i_win, did_i_tie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rush agent on the right side\n",
    "class Rush_Agent_Player_1(Agent):\n",
    "    def __init__(self, init_observation : Observation, action_space : Dict[ActionName, ActionSpace]) -> None: \n",
    "        super().__init__(init_observation, action_space)\n",
    "\n",
    "    def take_turn(self, current_game_state : Observation) -> Mapping[ActionName, Action]:\n",
    "        mapping = {}\n",
    "        if current_game_state.features['Tensor'][0][2] > 0 :\n",
    "            #rush\n",
    "            mapping['Move'] = GlobalCategoricalAction(0, self.action_space['Move'].index_to_label[0])\n",
    "        else:\n",
    "        #     #advance\n",
    "            mapping[\"Move\"] = GlobalCategoricalAction(1, self.action_space['Move'].index_to_label[2])\n",
    "        return mapping\n",
    "    \n",
    "    def on_game_start(self) -> None:\n",
    "        return super().on_game_start()\n",
    "    \n",
    "    def on_game_over(self, did_i_win : bool, did_i_tie : bool) -> None:\n",
    "        return super().on_game_over(did_i_win, did_i_tie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#includes rush and boom\n",
    "class Random_Agent(Agent):\n",
    "    def __init__(self, init_observation : Observation, action_space : Dict[ActionName, ActionSpace]) -> None: \n",
    "        super().__init__(init_observation, action_space)\n",
    "\n",
    "    def take_turn(self, current_game_state : Observation) -> Mapping[ActionName, Action]:\n",
    "        mapping = {}\n",
    "\n",
    "        action_choice = random.randrange(0, 4)\n",
    "        mapping['Move'] = GlobalCategoricalAction(0, self.action_space['Move'].index_to_label[action_choice])\n",
    "        return mapping\n",
    "    \n",
    "    def on_game_start(self) -> None:\n",
    "        return super().on_game_start()\n",
    "    \n",
    "    def on_game_over(self, did_i_win : bool, did_i_tie : bool) -> None:\n",
    "        return super().on_game_over(did_i_win, did_i_tie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some useful functions\n",
    "import functools\n",
    "def flatten_dict_of_arrays(data, prefix=\"\"):\n",
    "  flattened_list = []\n",
    "  def flatten_inner(value, key):\n",
    "    if isinstance(value, list):\n",
    "      for i, v in enumerate(value):\n",
    "        flatten_inner(v, f\"{prefix}{key}[{i}]\")\n",
    "    else:\n",
    "      flattened_list.append((f\"{prefix}{key}\", value))\n",
    "\n",
    "  for key, value in data.items():\n",
    "    flatten_inner(value, key)\n",
    "\n",
    "  return [value for _, value in flattened_list]\n",
    "\n",
    "\n",
    "def value_to_discrete_4(input):\n",
    "    output = [0,0,0,0]\n",
    "    output[input] = 1\n",
    "    return output\n",
    "\n",
    "index_to_moves = {\n",
    "    0 : \"advance\",\n",
    "    1 : \"retreat\",\n",
    "    2 : \"rush\",\n",
    "    3 : \"boom\"\n",
    "}\n",
    "moves_to_index = {\n",
    "    \"advance\": 0,\n",
    "    \"retreat\": 1,\n",
    "    \"rush\": 2,\n",
    "    \"boom\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewritten gameRunner to save game state sequence\n",
    "class GameRunnerSaveStates(): \n",
    "    def __init__(self, environment = None, enable_printouts : bool = False):\n",
    "        self.game = Interactive_TensorRTS(enable_printouts=enable_printouts)\n",
    "        self.game.reset()\n",
    "\n",
    "        self.player_one = None\n",
    "        self.player_two = None\n",
    "        self.results : GameResult = None\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def assign_players(self, first_agent : Agent, second_agent : Agent = None):\n",
    "        self.player_one = first_agent\n",
    "\n",
    "        if second_agent is not None:\n",
    "            self.player_two = second_agent\n",
    "\n",
    "    def run(self): \n",
    "        assert(self.player_one is not None)\n",
    "\n",
    "        game_state = self.game.observe()\n",
    "        self.player_one.on_game_start()\n",
    "        if self.player_two is not None: \n",
    "            self.player_two.on_game_start()\n",
    "\n",
    "        while(self.game.is_game_over is False):\n",
    "            #take moves and pass updated environments to agents\n",
    "\n",
    "            self.observations.append(flatten_dict_of_arrays(game_state.features))\n",
    "            action = self.player_one.take_turn(game_state)\n",
    "            self.actions.append(value_to_discrete_4(action[\"Move\"].index))\n",
    "\n",
    "            game_state = self.game.act(action)\n",
    "            self.peepee = game_state\n",
    "\n",
    "            self.rewards.append(game_state.reward)\n",
    "            self.dones.append(game_state.done)\n",
    "            if (self.game.is_game_over is False):\n",
    "                if self.player_two is None: \n",
    "                    game_state = self.game.opponent_act()\n",
    "                else:\n",
    "                    #future player_two code\n",
    "                    game_state = self.game.act(self.player_two.take_turn(game_state), False, True)\n",
    "\n",
    "        # who won? \n",
    "        tie = False\n",
    "        win_p_one = False\n",
    "        win_p_two = False\n",
    "\n",
    "        p_one = self.game.tensor_power(0)\n",
    "        p_two = self.game.tensor_power(1)\n",
    "\n",
    "        if p_one > p_two: \n",
    "            win_p_one = True\n",
    "        elif p_two > p_one:\n",
    "            win_p_two = True\n",
    "        else:\n",
    "            tie = True\n",
    "\n",
    "        self.results = GameResult(win_p_one, win_p_two, tie)\n",
    "        self.player_one.on_game_over(win_p_one, tie)\n",
    "        if self.player_two is not None:\n",
    "            self.player_two.on_game_over(win_p_two, tie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_runs,agent1,agent2):\n",
    "    datasetDict = {\n",
    "        'observations': [],\n",
    "        'actions': [], \n",
    "        'rewards': [], \n",
    "        'dones': [],\n",
    "    }\n",
    "\n",
    "    #generates training data\n",
    "    for _ in range(num_runs):\n",
    "        game_runner = GameRunnerSaveStates(enable_printouts=False)\n",
    "\n",
    "        observation = game_runner.game.observe()\n",
    "        action_space = game_runner.game.action_space()\n",
    "\n",
    "        player1 = agent1(observation,action_space)\n",
    "        player2 = agent2(observation,action_space)\n",
    "\n",
    "        game_runner.assign_players(player1, player2)\n",
    "        game_runner.run()\n",
    "\n",
    "        datasetDict['observations'].append(game_runner.observations)\n",
    "        datasetDict['actions'].append(game_runner.actions)\n",
    "        datasetDict['rewards'].append(game_runner.rewards)\n",
    "        datasetDict['dones'].append(game_runner.dones)\n",
    "\n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_dict(datasetDict)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on two rush bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rushDataset = generate_dataset(1000,Rush_Agent_Player_1,Rush_Agent_Player_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 5, 8, 8, 12, 1, 19, 1, 23, 8, 28, 5, 13, 1, 2, 0, 18, 1, 2, 0],\n",
       " [3, 5, 8, 8, 12, 1, 19, 1, 23, 8, 28, 5, 14, 1, 2, 0, 17, 1, 2, 0],\n",
       " [3, 5, 8, 8, 12, 1, 19, 1, 23, 8, 28, 5, 15, 1, 2, 0, 16, 1, 2, 0]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rushDataset[2]['observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e32f19e557e453a8ff90c7c01fe1800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0629, 'learning_rate': 6.25e-05, 'epoch': 31.25}\n",
      "{'loss': 0.0004, 'learning_rate': 9.722222222222223e-05, 'epoch': 62.5}\n",
      "{'loss': 0.0002, 'learning_rate': 9.027777777777779e-05, 'epoch': 93.75}\n",
      "{'loss': 0.0001, 'learning_rate': 8.333333333333334e-05, 'epoch': 125.0}\n",
      "{'loss': 0.0, 'learning_rate': 7.638888888888889e-05, 'epoch': 156.25}\n",
      "{'loss': 0.0, 'learning_rate': 6.944444444444444e-05, 'epoch': 187.5}\n",
      "{'loss': 0.0, 'learning_rate': 6.25e-05, 'epoch': 218.75}\n",
      "{'loss': 0.0, 'learning_rate': 5.555555555555556e-05, 'epoch': 250.0}\n",
      "{'loss': 0.0, 'learning_rate': 4.8611111111111115e-05, 'epoch': 281.25}\n",
      "{'loss': 0.0, 'learning_rate': 4.166666666666667e-05, 'epoch': 312.5}\n",
      "{'loss': 0.0, 'learning_rate': 3.472222222222222e-05, 'epoch': 343.75}\n",
      "{'loss': 0.0, 'learning_rate': 2.777777777777778e-05, 'epoch': 375.0}\n",
      "{'loss': 0.0, 'learning_rate': 2.0833333333333336e-05, 'epoch': 406.25}\n",
      "{'loss': 0.0, 'learning_rate': 1.388888888888889e-05, 'epoch': 437.5}\n",
      "{'loss': 0.0, 'learning_rate': 6.944444444444445e-06, 'epoch': 468.75}\n",
      "{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 500.0}\n",
      "{'train_runtime': 758.1363, 'train_samples_per_second': 659.512, 'train_steps_per_second': 10.552, 'train_loss': 0.003982970461205696, 'epoch': 500.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DecisionTransformerConfig, Trainer, TrainingArguments\n",
    "\n",
    "import DTscripts\n",
    "\n",
    "collator = DTscripts.DecisionTransformerGymDataCollator(rushDataset)\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = DTscripts.TrainableDT(config)\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=500,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "    use_mps_device=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps = 15 #needs a evaluation dataset \n",
    "\n",
    ")\n",
    "training_args = training_args.set_save(strategy=\"steps\", steps=500)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=rushDataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./output/rush\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TensorRTS import TensorRTS\n",
    "\n",
    "\n",
    "def get_move_no_lookback(model,state):\n",
    "    state_dim = 20\n",
    "    act_dim = 4\n",
    "    states = torch.from_numpy(state).reshape(1, 1, state_dim).to(device=device, dtype=torch.float32)\n",
    "    actions = torch.zeros((1, 1, act_dim), device=device, dtype=torch.float32)\n",
    "    rewards = torch.zeros(1, 1, device=device, dtype=torch.float32)\n",
    "    target_return = torch.tensor(10, dtype=torch.float32).reshape(1, 1)\n",
    "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "    attention_mask = torch.zeros(1, 1, device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        state_preds, action_preds, return_preds = model(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            returns_to_go=target_return,\n",
    "            timesteps=timesteps,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False,\n",
    "            # use_mps_device=True\n",
    "\n",
    "        )\n",
    "    return state_preds, action_preds, return_preds\n",
    "\n",
    "def evaluate_model(model,num_runs):\n",
    "    testEnv = TensorRTS(enable_prinouts = False)\n",
    "    possible_moves = {\n",
    "        0 : \"advance\",\n",
    "        1 : \"retreat\",\n",
    "        2 : \"rush\",\n",
    "        3 : \"boom\"\n",
    "    }\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    ties = 0\n",
    "    for _ in range(num_runs):\n",
    "\n",
    "        testEnv.reset()\n",
    "        for _ in range(1000):\n",
    "            action = {}\n",
    "            #make prediction\n",
    "            model_out = get_move_no_lookback(model,np.array(flatten_dict_of_arrays(testEnv.observe().features)))\n",
    "            # print(model_out[1])\n",
    "            # if possible_moves[torch.argmax(model_out[1]).item()] == \"advance\" : print(\"advance\")\n",
    "            # print(torch.argmax(model_out[1]).item())\n",
    "            action['Move'] = GlobalCategoricalAction(0, possible_moves[torch.argmax(model_out[1]).item()])\n",
    "            pred_reward = model_out[2]\n",
    "            result = testEnv.act(action)\n",
    "            # print(model_out[1])\n",
    "\n",
    "            if result.done == True: \n",
    "                break \n",
    "            \n",
    "\n",
    "        # print(result.reward)\n",
    "        if (result.reward == 10): \n",
    "            wins+=1\n",
    "        if (result.reward == 0): ties +=1\n",
    "        if (result.reward == -10): losses+=1\n",
    "\n",
    "\n",
    "    return ((wins+ties)/num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DecisionTransformerModel\n",
    "\n",
    "model = DecisionTransformerModel.from_pretrained(\"./output/rush\")\n",
    "\n",
    "evaluate_model(model,100) #its not good :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
